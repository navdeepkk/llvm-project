//
// Generated by LLVM NVPTX Back-End
//

.version 6.3
.target sm_75
.address_size 64

	// .globl	main_kernel
// __wg_main_kernel_0 has been demoted
// __wg_main_kernel_1 has been demoted
.shared .align 4 .b8 __wg_main_kernel_2[32];

.visible .entry main_kernel(
	.param .u64 main_kernel_param_0,
	.param .u64 main_kernel_param_1,
	.param .u64 main_kernel_param_2,
	.param .u64 main_kernel_param_3,
	.param .u64 main_kernel_param_4,
	.param .u64 main_kernel_param_5,
	.param .u64 main_kernel_param_6,
	.param .u64 main_kernel_param_7,
	.param .u64 main_kernel_param_8,
	.param .u64 main_kernel_param_9,
	.param .u64 main_kernel_param_10,
	.param .u64 main_kernel_param_11
)
{
	.local .align 4 .b8 	__local_depot0[144];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b16 	%h<49>;
	.reg .b32 	%hh<41>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<15>;
	// demoted variable
	.shared .align 2 .b8 __wg_main_kernel_0[2048];
	// demoted variable
	.shared .align 2 .b8 __wg_main_kernel_1[2048];
	mov.u64 	%SPL, __local_depot0;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd3, [main_kernel_param_8];
	mov.u32 	%r1, %ctaid.x;
	cvt.s64.s32 	%rd1, %r1;
	mov.u32 	%r2, %tid.x;
	cvt.s64.s32 	%rd2, %r2;
	mul.lo.s64 	%rd4, %rd1, 6;
	add.s64 	%rd5, %rd4, %rd2;
	shl.b64 	%rd6, %rd5, 2;
	add.u64 	%rd7, %SP, 96;
	add.s64 	%rd8, %rd7, %rd6;
	ld.u32 	%r3, [%rd8];
	mov.u64 	%rd9, __wg_main_kernel_0;
	add.s64 	%rd10, %rd9, 1056;
	mov.u32 	%r4, 32;
	wmma.load.a.sync.aligned.row.m16n16k16.shared.f16 	{%hh1, %hh2, %hh3, %hh4, %hh5, %hh6, %hh7, %hh8}, [%rd10], %r4;
	mov.b32 	{%h1, %h2}, %hh1;
	st.v2.b16 	[%SP+0], {%h1, %h2};
	mov.b32 	{%h3, %h4}, %hh2;
	st.v2.b16 	[%SP+4], {%h3, %h4};
	mov.b32 	{%h5, %h6}, %hh3;
	st.v2.b16 	[%SP+8], {%h5, %h6};
	mov.b32 	{%h7, %h8}, %hh4;
	st.v2.b16 	[%SP+12], {%h7, %h8};
	mov.b32 	{%h9, %h10}, %hh5;
	st.v2.b16 	[%SP+16], {%h9, %h10};
	mov.b32 	{%h11, %h12}, %hh6;
	st.v2.b16 	[%SP+20], {%h11, %h12};
	mov.b32 	{%h13, %h14}, %hh7;
	st.v2.b16 	[%SP+24], {%h13, %h14};
	mov.b32 	{%h15, %h16}, %hh8;
	st.v2.b16 	[%SP+28], {%h15, %h16};
	wmma.load.b.sync.aligned.row.m16n16k16.shared.f16 	{%hh9, %hh10, %hh11, %hh12, %hh13, %hh14, %hh15, %hh16}, [%rd10], %r4;
	mov.b32 	{%h17, %h18}, %hh9;
	st.v2.b16 	[%SP+32], {%h17, %h18};
	mov.b32 	{%h19, %h20}, %hh10;
	st.v2.b16 	[%SP+36], {%h19, %h20};
	mov.b32 	{%h21, %h22}, %hh11;
	st.v2.b16 	[%SP+40], {%h21, %h22};
	mov.b32 	{%h23, %h24}, %hh12;
	st.v2.b16 	[%SP+44], {%h23, %h24};
	mov.b32 	{%h25, %h26}, %hh13;
	st.v2.b16 	[%SP+48], {%h25, %h26};
	mov.b32 	{%h27, %h28}, %hh14;
	st.v2.b16 	[%SP+52], {%h27, %h28};
	mov.b32 	{%h29, %h30}, %hh15;
	st.v2.b16 	[%SP+56], {%h29, %h30};
	mov.b32 	{%h31, %h32}, %hh16;
	st.v2.b16 	[%SP+60], {%h31, %h32};
	wmma.load.c.sync.aligned.row.m16n16k16.shared.f16 	{%hh17, %hh18, %hh19, %hh20}, [%rd10], %r4;
	mov.b32 	{%h33, %h34}, %hh17;
	st.v2.b16 	[%SP+64], {%h33, %h34};
	mov.b32 	{%h35, %h36}, %hh18;
	st.v2.b16 	[%SP+68], {%h35, %h36};
	mov.b32 	{%h37, %h38}, %hh19;
	st.v2.b16 	[%SP+72], {%h37, %h38};
	mov.b32 	{%h39, %h40}, %hh20;
	st.v2.b16 	[%SP+76], {%h39, %h40};
	ld.b32 	%hh21, [%SP+0];
	ld.b32 	%hh22, [%SP+4];
	ld.b32 	%hh23, [%SP+8];
	ld.b32 	%hh24, [%SP+12];
	ld.b32 	%hh25, [%SP+16];
	ld.b32 	%hh26, [%SP+20];
	ld.b32 	%hh27, [%SP+24];
	ld.b32 	%hh28, [%SP+28];
	ld.b32 	%hh29, [%SP+32];
	ld.b32 	%hh30, [%SP+36];
	ld.b32 	%hh31, [%SP+40];
	ld.b32 	%hh32, [%SP+44];
	ld.b32 	%hh33, [%SP+48];
	ld.b32 	%hh34, [%SP+52];
	ld.b32 	%hh35, [%SP+56];
	ld.b32 	%hh36, [%SP+60];
	wmma.mma.sync.aligned.row.row.m16n16k16.f16.f16
		{%hh37, %hh38, %hh39, %hh40},
		{%hh21, %hh22, %hh23, %hh24, %hh25, %hh26, %hh27, %hh28},
		{%hh29, %hh30, %hh31, %hh32, %hh33, %hh34, %hh35, %hh36},
		{%hh17, %hh18, %hh19, %hh20};
	mov.b32 	{%h41, %h42}, %hh37;
	st.v2.b16 	[%SP+80], {%h41, %h42};
	mov.b32 	{%h43, %h44}, %hh38;
	st.v2.b16 	[%SP+84], {%h43, %h44};
	mov.b32 	{%h45, %h46}, %hh39;
	st.v2.b16 	[%SP+88], {%h45, %h46};
	mov.b32 	{%h47, %h48}, %hh40;
	st.v2.b16 	[%SP+92], {%h47, %h48};
	mov.u64 	%rd11, __wg_main_kernel_1;
	add.s64 	%rd12, %rd11, 1056;
	wmma.store.d.sync.aligned.row.m16n16k16.shared.f16 	[%rd12],{%hh37, %hh38, %hh39, %hh40}, %r4;
	shl.b64 	%rd13, %rd1, 2;
	add.s64 	%rd14, %rd3, %rd13;
	st.u32 	[%rd14], %r3;
	ret;

}
